import { useCallback, useEffect, useRef, useState } from 'react';
import { IWebConnection } from '../connection/types';
import { useAudioVisualizer } from './useAudioVisualizer';

interface VoiceChatState {
  isVoiceEnabled: boolean;
  isMuted: boolean;
  isRemoteVoiceActive: boolean;
  localAudioStream: MediaStream | null;
  remoteAudioStream: MediaStream | null;
  error: string | null;
}

export function useVoiceChatBusiness(connection: IWebConnection) {
  const [state, setState] = useState<VoiceChatState>({
    isVoiceEnabled: false,
    isMuted: false,
    isRemoteVoiceActive: false,
    localAudioStream: null,
    remoteAudioStream: null,
    error: null,
  });

  const localAudioStreamRef = useRef<MediaStream | null>(null);
  const audioSenderRef = useRef<RTCRtpSender | null>(null);
  const remoteAudioRef = useRef<HTMLAudioElement | null>(null);

  // ä½¿ç”¨éŸ³é¢‘å¯è§†åŒ–
  const localAudioVisualizer = useAudioVisualizer(state.localAudioStream);
  const remoteAudioVisualizer = useAudioVisualizer(state.remoteAudioStream);

  const updateState = useCallback((updates: Partial<VoiceChatState>) => {
    setState(prev => ({ ...prev, ...updates }));
  }, []);

  // ç›‘å¬è¿œç¨‹éŸ³é¢‘è½¨é“
  const handleRemoteAudioTrack = useCallback((event: RTCTrackEvent, currentTrackRef: { current: MediaStreamTrack | null }) => {
    if (event.track.kind !== 'audio') return;
    
    // ç§»é™¤æ—§è½¨é“çš„ç›‘å¬å™¨
    if (currentTrackRef.current) {
      currentTrackRef.current.onended = null;
      currentTrackRef.current.onmute = null;
      currentTrackRef.current.onunmute = null;
    }
    currentTrackRef.current = event.track;
    
    if (event.streams.length > 0) {
      const remoteStream = event.streams[0];
      event.track.enabled = true;
      
      // æ›´æ–°çŠ¶æ€
      setState(prev => ({ 
        ...prev, 
        remoteAudioStream: remoteStream,
        isRemoteVoiceActive: true 
      }));

      // ç›‘å¬è½¨é“ç»“æŸäº‹ä»¶
      event.track.onended = () => {
        setState(prev => ({ ...prev, isRemoteVoiceActive: false }));
      };
      
      // ç›‘å¬è½¨é“é™éŸ³äº‹ä»¶
      event.track.onmute = () => {
        // è¿œç¨‹éŸ³é¢‘è½¨é“è¢«é™éŸ³
      };
      
      event.track.onunmute = () => {
        // è¿œç¨‹éŸ³é¢‘è½¨é“å–æ¶ˆé™éŸ³
      };

      // åœ¨è®¾ç½®çŠ¶æ€åï¼Œä½¿ç”¨ setTimeout ç¡®ä¿ audio å…ƒç´ æ›´æ–°
      setTimeout(() => {
        if (remoteAudioRef.current && remoteStream.active) {
          remoteAudioRef.current.srcObject = remoteStream;
          remoteAudioRef.current.play().catch(err => {
            // å¿½ç•¥ AbortErrorï¼Œè¿™æ˜¯æ­£å¸¸çš„ç«æ€æ¡ä»¶
            if (err.name !== 'AbortError') {
              console.error('[VoiceChat] æ’­æ”¾è¿œç¨‹éŸ³é¢‘å¤±è´¥:', err);
            }
          });
        }
      }, 0);
    }
  }, []); // ç©ºä¾èµ–æ•°ç»„ï¼Œå‡½æ•°å¼•ç”¨å§‹ç»ˆä¸å˜

  useEffect(() => {
    if (!connection) return;
    
    const currentTrackRef = { current: null as MediaStreamTrack | null };

    const trackHandler = (event: RTCTrackEvent) => {
      if (event.track.kind === 'audio') {
        handleRemoteAudioTrack(event, currentTrackRef);
      }
    };

    const cleanup = connection.onTrack(trackHandler);
    
    return () => {
      if (currentTrackRef.current) {
        currentTrackRef.current.onended = null;
        currentTrackRef.current.onmute = null;
        currentTrackRef.current.onunmute = null;
      }
      if (cleanup) {
        cleanup();
      }
    };
  }, [connection, handleRemoteAudioTrack]); // åªåœ¨ connection æˆ–å¤„ç†å™¨å˜åŒ–æ—¶é‡æ–°æ³¨å†Œ

  // è·å–æœ¬åœ°éŸ³é¢‘æµ
  const getLocalAudioStream = useCallback(async (): Promise<MediaStream> => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
        video: false,
      });

      return stream;
    } catch (error) {
      console.error('[VoiceChat] è·å–æœ¬åœ°éŸ³é¢‘æµå¤±è´¥:', error);
      
      // æ ¹æ®é”™è¯¯ç±»å‹æä¾›æ›´è¯¦ç»†çš„é”™è¯¯æ¶ˆæ¯
      if (error instanceof DOMException) {
        if (error.name === 'NotAllowedError') {
          throw new Error('éº¦å…‹é£æƒé™è¢«æ‹’ç»ï¼Œè¯·åœ¨æµè§ˆå™¨è®¾ç½®ä¸­å…è®¸ä½¿ç”¨éº¦å…‹é£');
        } else if (error.name === 'NotFoundError') {
          throw new Error('æœªæ£€æµ‹åˆ°éº¦å…‹é£è®¾å¤‡ï¼Œè¯·è¿æ¥éº¦å…‹é£åé‡è¯•');
        } else if (error.name === 'NotReadableError') {
          throw new Error('éº¦å…‹é£è¢«å…¶ä»–åº”ç”¨å ç”¨ï¼Œè¯·å…³é—­å…¶ä»–ä½¿ç”¨éº¦å…‹é£çš„ç¨‹åº');
        } else if (error.name === 'OverconstrainedError') {
          throw new Error('éº¦å…‹é£ä¸æ”¯æŒæ‰€éœ€çš„éŸ³é¢‘è®¾ç½®');
        } else if (error.name === 'AbortError') {
          throw new Error('éº¦å…‹é£è®¿é—®è¢«ä¸­æ–­');
        } else if (error.name === 'SecurityError') {
          throw new Error('å®‰å…¨é™åˆ¶ï¼šæ— æ³•è®¿é—®éº¦å…‹é£ï¼ˆè¯·ä½¿ç”¨HTTPSï¼‰');
        }
      }
      
      throw new Error('æ— æ³•è·å–éº¦å…‹é£æƒé™ï¼Œè¯·ç¡®ä¿å…è®¸ä½¿ç”¨éº¦å…‹é£');
    }
  }, []);
        
  // å¯ç”¨è¯­éŸ³é€šè¯
  const enableVoice = useCallback(async () => {
    if (state.isVoiceEnabled || !connection) {
      return;
    }

    try {
      updateState({ error: null });

      // æ£€æŸ¥P2Pè¿æ¥çŠ¶æ€
      const connectState = connection.getConnectState();
      if (!connectState.isPeerConnected) {
        throw new Error('P2Pè¿æ¥å°šæœªå»ºç«‹ï¼Œæ— æ³•å¯ç”¨è¯­éŸ³');
      }

      // è·å–æœ¬åœ°éŸ³é¢‘æµ
      const stream = await getLocalAudioStream();
      localAudioStreamRef.current = stream;
      
      console.log('[VoiceChat] âœ… æœ¬åœ°éŸ³é¢‘æµè·å–æˆåŠŸ:', {
        streamId: stream.id,
        audioTracks: stream.getAudioTracks().length,
        trackEnabled: stream.getAudioTracks()[0]?.enabled,
        trackReadyState: stream.getAudioTracks()[0]?.readyState
      });

      // æ·»åŠ éŸ³é¢‘è½¨é“åˆ°P2Pè¿æ¥
      const audioTrack = stream.getAudioTracks()[0];
      if (audioTrack) {
        const role = connection.currentRoom?.role;
        console.log('[VoiceChat] ğŸ“¤ æ·»åŠ éŸ³é¢‘è½¨é“åˆ°P2Pè¿æ¥, å½“å‰è§’è‰²:', role);
        
        const sender = connection.addTrack(audioTrack, stream);
        audioSenderRef.current = sender;
        
        if (sender) {
          console.log('[VoiceChat] ğŸ“Š Sender ä¿¡æ¯:', {
            track: sender.track?.id,
            trackEnabled: sender.track?.enabled,
            trackReadyState: sender.track?.readyState
          });
        }
        
        // é‡è¦ï¼šæ·»åŠ éŸ³é¢‘è½¨é“åï¼Œæœ¬åœ°å¿…é¡»ä¸»åŠ¨åˆ›å»º offer
        // å› ä¸ºå¯¹æ–¹ä¸çŸ¥é“æˆ‘ä»¬æ·»åŠ äº†æ–°è½¨é“ï¼Œå¿…é¡»ç”±æˆ‘ä»¬é€šçŸ¥å¯¹æ–¹
        console.log('[VoiceChat] ğŸ“¡ [' + role + '] åˆ›å»º offer è¿›è¡Œé‡æ–°åå•†ï¼ˆæ·»åŠ éŸ³é¢‘è½¨é“ï¼‰');
        const negotiated = await connection.createOfferNow();
        console.log('[VoiceChat] ğŸ“¡ [' + role + '] é‡æ–°åå•†ç»“æœ:', negotiated);
      }

      updateState({
        isVoiceEnabled: true,
        localAudioStream: stream,
        isMuted: false,
      });
    } catch (error) {
      console.error('[VoiceChat] å¯ç”¨è¯­éŸ³å¤±è´¥:', error);
      const errorMsg = error instanceof Error ? error.message : 'å¯ç”¨è¯­éŸ³å¤±è´¥';
      updateState({ error: errorMsg });
      throw error;
    }
  }, [connection, getLocalAudioStream, state.isVoiceEnabled, updateState]);

  // ç¦ç”¨è¯­éŸ³é€šè¯
  const disableVoice = useCallback(async () => {
    if (!state.isVoiceEnabled) return;

    const role = connection.currentRoom?.role;

    // ç§»é™¤éŸ³é¢‘è½¨é“
    if (audioSenderRef.current) {
      connection.removeTrack(audioSenderRef.current);
      audioSenderRef.current = null;
      
      // é‡è¦ï¼šç§»é™¤éŸ³é¢‘è½¨é“åï¼Œæœ¬åœ°å¿…é¡»ä¸»åŠ¨åˆ›å»º offer
      // å› ä¸ºå¯¹æ–¹ä¸çŸ¥é“æˆ‘ä»¬ç§»é™¤äº†è½¨é“ï¼Œå¿…é¡»ç”±æˆ‘ä»¬é€šçŸ¥å¯¹æ–¹
      console.log('[VoiceChat] ğŸ“¡ [' + role + '] ç§»é™¤éŸ³é¢‘è½¨é“åé‡æ–°åå•†');
      try {
        await connection.createOfferNow();
      } catch (error) {
        console.error('[VoiceChat] é‡æ–°åå•†å¤±è´¥:', error);
      }
    }

    // åœæ­¢æœ¬åœ°éŸ³é¢‘æµ
    if (localAudioStreamRef.current) {
      localAudioStreamRef.current.getTracks().forEach(track => {
        track.stop();
      });
      localAudioStreamRef.current = null;
    }

    updateState({
      isVoiceEnabled: false,
      localAudioStream: null,
      isMuted: false,
    });
  }, [connection, state.isVoiceEnabled, updateState]);

  // åˆ‡æ¢é™éŸ³çŠ¶æ€
  const toggleMute = useCallback(() => {
    if (!localAudioStreamRef.current) {
      return;
    }

    const audioTracks = localAudioStreamRef.current.getAudioTracks();
    if (audioTracks.length === 0) {
      return;
    }

    const newMutedState = !state.isMuted;
    audioTracks.forEach(track => {
      track.enabled = !newMutedState;
    });

    updateState({ isMuted: newMutedState });
  }, [state.isMuted, updateState]);

  // è®¾ç½®è¿œç¨‹éŸ³é¢‘å…ƒç´ å¼•ç”¨
  const setRemoteAudioRef = useCallback((element: HTMLAudioElement | null) => {
    remoteAudioRef.current = element;
    if (element && state.remoteAudioStream && state.remoteAudioStream.active) {
      element.srcObject = state.remoteAudioStream;
      element.play().catch(err => {
        // å¿½ç•¥ AbortErrorï¼Œè¿™æ˜¯æ­£å¸¸çš„ç«æ€æ¡ä»¶
        if (err.name !== 'AbortError') {
          console.error('[VoiceChat] æ’­æ”¾è¿œç¨‹éŸ³é¢‘å¤±è´¥:', err);
        }
      });
    }
  }, [state.remoteAudioStream]);

  // æ¸…ç†
  useEffect(() => {
    return () => {
      if (localAudioStreamRef.current) {
        localAudioStreamRef.current.getTracks().forEach(track => track.stop());
      }
    };
  }, []);

  return {
    // çŠ¶æ€
    isVoiceEnabled: state.isVoiceEnabled,
    isMuted: state.isMuted,
    isRemoteVoiceActive: state.isRemoteVoiceActive,
    error: state.error,
    
    // éŸ³é¢‘å¯è§†åŒ–æ•°æ®
    localVolume: localAudioVisualizer.volume,
    localIsSpeaking: localAudioVisualizer.isSpeaking,
    remoteVolume: remoteAudioVisualizer.volume,
    remoteIsSpeaking: remoteAudioVisualizer.isSpeaking,
    
    // æ–¹æ³•
    enableVoice,
    disableVoice,
    toggleMute,
    setRemoteAudioRef,
    
    // è°ƒè¯•ä¿¡æ¯
    _debug: {
      hasRemoteStream: !!state.remoteAudioStream,
      remoteStreamId: state.remoteAudioStream?.id,
      remoteTrackCount: state.remoteAudioStream?.getTracks().length || 0,
    }
  };
}
